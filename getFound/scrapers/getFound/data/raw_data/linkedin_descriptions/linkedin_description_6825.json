{
    "https://www.linkedin.com/jobs/view/machine-learning-research-scientist-at-d-matrix-3611883765?refId=WSd9dFaR29zbgb%2FFsSCmgQ%3D%3D&trackingId=JxTmWcDgm4kiLWU%2FpeWYpw%3D%3D&position=20&pageNum=0&trk=public_jobs_jserp-result_search-card": "\nMACHINE LEARNING RESEARCH SCIENTISTAbout UsIf you are following the evolution of the leading approach in deep learning powered AI, the renaissance in NLP as well as the next disruption in computer vision, you likely know it’s all about Transformer based models.. They are powering neural nets with billions to trillions of parameters and existing silicon architectures (including the plethora of AI accelerators) are struggling to varying degrees to keep up with exploding model sizes and their performance requirements. More importantly, TCO considerations for running these models at scale are becoming a bottleneck to meet exploding demand. Hyperscalers are keen on how to gain COGS efficiencies with the trillions of AI inferences/day they are already serving, but certainly for addressing the steep demand ramp they are anticipating in the next couple of years. d-Matrix is addressing this problem head on by developing a fully digital in memory computing accelerator for AI inference that is highly optimized for the computational patterns in Transformers. The fully digital approach removes some of the difficulties of analog techniques that are most often touted in pretty much all other in-memory computing AI inference products. d-Matrix’s AI inference accelerator has also been architected as a chiplet, thereby enabling both a scale-up and scale-out solution with flexible packaging options. The d-Matrix team has a stellar track record in developing and commercializing silicon at scale as senior execs at the likes of Inphi, Broadcom, and Intel. Notably, they recognized early the extremely important role of programmability and the software stack and are thoughtfully building up the team in this area even since before their Series A. The company has raised $44m in funding so far and has 70+ employees across Silicon Valley, Sydney and Bengaluru.Why d-MatrixWe want to build a company and a culture that sustains the tests of time. We offer the candidate a very unique opportunity to express themselves and become a future leader in an industry that will have a huge influence globally. We are striving to build a culture of transparency, inclusiveness and intellectual honesty while ensuring all our team members are always learning and having fun on the journey. We have built the industry’s first highly programmable in-memory computing architecture that applies to a broad class of applications from cloud to edge. The candidate will get to work on a path breaking architecture with a highly experienced team that knows what it takes to build a successful business.The role: Machine Learning Research ScientistThe Machine Learning Team is responsible for the R&D of core algorithm-hardware co-design capabilities in d-Matrix's end-to-end solution. You will be joining a team of exceptional people enthusiastic about researching and developing state-of-the-art efficient deep learning techniques tailored for d-Matrix's AI compute engine. You will also have the opportunity of collaboration with top academic labs and help customers to optimize and deploy workloads for real-world AI applications on our systems.Responsibilities Design, implement and evaluate efficient deep neural network architectures and algorithms for d-Matrix's AI compute engine. Engage and collaborate with internal and external ML researchers to meet R&D goals. Engage and collaborate with SW team to meet stack development milestones. Conduct research to guide hardware design. Develop and maintain tools for high-level simulation and research. Port customer workloads, optimize them for deployment, generate reference implementations and evaluate performance. Report and present progress timely and effectively. Contribute to publications of papers and intellectual properties.QualificationsMinimum: PhD in Computer Science, Electrical and Computer Engineering, or a related scientific discipline. High proficiency with major deep learning frameworks: PyTorch, TensorFlow is a must. High proficiency in algorithm analysis, data structure, and Python programming is a must. Deep, wide and current knowledge in machine learning and modern deep learning. Strong publication record in top machine learning conferences or journals. Hands-on experience with CNN, RNN, Transformer neural network architectures. Knowledge and experience with efficient deep learning is preferred: quantization, sparsity, distillation.Desired: Proficiency with C/C++ programming is preferred. Proficiency with GPU CUDA programming is preferred. Experience with AutoML and meta learning is preferred. Experience with numerical analysis preferred. Experience with specialized HW accelerator systems for deep neural network is preferred. Passionate about AI and thriving in a fast-paced and dynamic startup culture.\n"
}